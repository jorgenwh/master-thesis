\textit{Graphical Processing Units} (GPUs) are massively parallel processing units designed for high-throughput parallel computations.
\textit{Central Processing Units}, on the other hand, are designed to perform many serial computations quickly, reserving more transistors to caching and predicting how programs will branch in the future.
GPUs were originally developed to accelerate computations performed on images, a highly parallel task where it is commonplace to have millions of relatively small independent computations that must be performed quickly in a single memory buffer.
Although GPUs have mainly been used for graphical computations, they have in recent years been adopted in other areas with the introduction of the \textit{General Purpose Graphical Processing Unit} (GPGPU).
The concept of the GPGPU is to use a GPU to accelerate computations in other domains where CPUs have traditionally been used.
Fields such as artificial intelligence and the broader scientific computing community have enjoyed great utility from GPUs, using them to accelerate embarrassingly parallel problems, e.g., matrix operations.
Despite being comparable in power consumption, a GPU can provide much higher instruction throughput and memory bandwidth compared to its CPU competitors.
These capability advantages exist in GPUs because they were specifically designed to perform well with regards to these dimensions.

While several distinct brands of GPUs exist, the work presented in this thesis only leverages GPUs produced by Nvidia, one of the leading accelerated computing manufacturers for scientific computing today \cite{gpu_marketshare}.
