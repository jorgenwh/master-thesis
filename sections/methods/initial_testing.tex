\subsection{Initial Testing} \label{methods:initial_testing}
Before delving into the GPU accelerated methods used to produce the final GKAGE product, we will here describe an initial test we performed in order to assess two things. Firstly, whether we could effectively GPU accelerate existing NumPy solutions using CuPy, all while staying within the comfort of Python, and secondly, whether such an implementation would result in significant speedup over a CPU solution or at least provide insight into how plausible getting significant speedup by utilizing the GPU was.
For this, we decided on focusing on the \textit{partial} \textit{k}mer counting problem, detailed in section \ref{background:kmers_and_kmer_counting}.

As detailed in section \ref{background:implementation_tools_and_libraries:npstructures}, npstructures is a Python library that enhances the NumPy library by providing additional data structures with NumPy-like behaviour and performance, built on top of NumPy.
Additionally in section \ref{background:implementation_tools_and_libraries:npstructures}, we detailed a subset of npstructures' data structures, one of which was a counter object that efficiently counts occurences of a predefined keys-set in a sample.
This counter object had previously been utilized to count a predefined set of \textit{k}mer's frequencies, albeit on the CPU.

Since npstructures' data structures are all heavily reliant upon NumPy's array routines for fast performance, they are designed around utilizing the CPU's vectorization and data parallelism.
Large array operations where data parallelism matters are ideal for the GPU architecture, which often can provide orders of magnitude more parallelism this way.
In addition, we know that most, if not all of the functionality used from NumPy, will be supported with GPU acceleration by CuPy through an nearly identical interface.
Thus, by replacing the NumPy functionality used in npstructures' data structures with CuPy's equivalent GPU accelerated functionality, we can GPU accelerate the necessary data structures in npstructures without having to leave Python.
With this strategy we GPU accelerated npstructures' counter object using CuPy.

\subsubsection{Implementation}
Rather than creating a standalone package version of npstructures with GPU acceleration, we instead opted to add the possibility for GPU acceleration to the already existing package.
This added a new self imposed requirement; we did not want CuPy to be a npstructures dependancy since many users may wish to use it simply for the NumPy implementations.
Additionally, we still needed a way to redirect NumPy function calls to their equivalent CuPy functions.
We fulfilled both of these requirements by exploiting Python's module system.

Consider the following Python package example where our package, \textit{mypackage}, contains two modules, \textit{my\_classes} and \textit{my\_funcs}, both relying on NumPy for their implementations:

\begin{center}
mypackage.my\_funcs.py
\end{center}
\begin{lstlisting}[language=Python,style=pycode]
import numpy as np

def some_func_using_numpy():
  return np.zeros(10)
\end{lstlisting}

\begin{center}
mypackage.my\_classes.py
\end{center}
\begin{lstlisting}[language=Python,style=pycode]
import numpy as np

class SomeClassUsingNumPy:
  def __init__(self):
    self.data = np.zeros(10)

  def get_data(self):
    return self.data
\end{lstlisting}

Our package's initialization file imports our function and our class from their respective modules, and all functionality is usable without needing to import CuPy in either module or initialization file.
Pay attention to the initialization file's \textit{set\_backend} function which takes a library as a parameter and reassigns the np variable in both package modules from the NumPy to the provided library.

\begin{center}
mypackage.\_\_init\_\_.py
\end{center}
\begin{lstlisting}[language=Python,style=pycode]
from .my_funcs import some_func_using_numpy
from .my_classes import SomeClassUsingNumPy 

# Swaps NumPy with lib (presumably CuPy)
def set_backend(lib):
  from . import my_funcs
  my_funcs.np = lib

  from . import my_classes
  my_classes.np = lib
\end{lstlisting}

In our own program, where we will import our package, \textit{mypackage}, we can either directly use our package's implementation with NumPy, or we can do as the following example shows and import CuPy and set the backend in the entire package to use CuPy functionality instead of NumPy.

\begin{center}
program.py
\end{center}
\begin{lstlisting}[language=Python,style=pycode]
import cupy as cp

import mypackage
mypackage.set_backend(cp)

array = mypackage.some_func_using_numpy()
type(array) # cupy.ndarray
\end{lstlisting}

Exploiting Python's module system this way has the benefits of not making CuPy a dependancy for npstructures, and it also allows for gradual GPU support by way of only updating the backend in modules where the existing implementations are ready to be ported as is to CuPy.

\subsubsection{Resolving Unsupported or Poorly Performing Functionality}
Two issues can arise when utilizing this method of GPU accelerating NumPy code.
Firstly, some NumPy solutions may be effective on the CPU's architecture but ineffective on the GPU's, resulting in poor performance.
This will often be the case when significant portions of the code needs to be ran in a serial fashion, as opposed to parallel, or when array sizes are too small to mask the overhead of copying data to and from the GPU memory.
Secondly, certain NumPy functions are simply not supported by CuPy.
In case of the former issue, one might want to create an alternative solution that better utilizes the GPU's strengths, such as its massive parallelism.
For the latter issue, there is no other practical option than to create a custom solution that achieves the desired behaviour by using what CuPy functionality is available.
We circumvented both of these issues by creating custom implementations where we had the freedom to use the full extend of CuPy's functionality to reproduce the desired behaviour.
For classes, this can be achieved by subclassing and overriding methods where we wish to create our new custom implementations.
To demonstrate this, we will have to slightly edit our example package, \textit{mypackage}.

Consider if our package's class definition had instead been the following:
\begin{center}
mypackage.my\_classes.py
\end{center}
\begin{lstlisting}[language=Python,style=pycode]
import numpy as np

class SomeClassUsingNumPy:
  def __init__(self):
    self.data = np.zeros(10)

  def pad_with_ones(self):
    arr = self.data
    self.data = np.insert(arr, [0, len(arr)], 1)
\end{lstlisting}

In the code above, we use NumPy's insert function, which as of April 2023 is not supported by CuPy.
To circumvent this issue, we can subclass our \textit{SomeClassUsingNumPy} class and create our own custom implementation of \textit{pad\_with\_ones}.
We will create a new file where we implement our CuPy compatible solution, and our subclass will override the \textit{pad\_with\_ones} method.
\begin{center}
mypackage.cp\_my\_classes.py
\end{center}
\begin{lstlisting}[language=Python,style=pycode]
import numpy as np
import cupy as cp

from .my_classes import SomeClassUsingNumPy

class CPSomeClassUsingNumPy(SomeClassUsingNumPy):
  def pad_with_ones(self):
    arr = self.data
    self.data = cp.pad(arr, (1, 1), 'constant', constant_values=1)
\end{lstlisting}

In the above example we override the \textit{pad\_with\_ones} method with our alternative implementation that uses a different CuPy function that is supported by CuPy, the \textit{pad} function.
Now, our two different \textit{pad\_with\_ones} implementations will behave equivalently, although our custom implementation will leverage CuPy and be GPU accelerated.

Finally, we need to make one small change to our package's initialization file so that users of our package will import our CuPy compatible subclass of \textit{SomeClassUsingNumPy} after setting the backend to CuPy:
\begin{center}
mypackage.\_\_init\_\_.py
\end{center}
\begin{lstlisting}[language=Python,style=pycode]
from .my_funcs import some_func_using_numpy
from .my_classes import SomeClassUsingNumPy 

# Swaps NumPy with lib (presumably CuPy)
def set_backend(lib):
  from . import my_funcs
  my_funcs.np = lib

  from . import my_classes
  my_classes.np = lib

  # Use CuPy compatible version of SomeClassUsingNumPy
  global SomeClassUsingNumPy
  from .cp_my_classes import CPSomeClassUsingNumPy
  SomeClassUsingNumPy = CPSomeClassUsingNumPy
\end{lstlisting}

By utilizing this method, we implemented partial GPU support for the npstructures library, including enabling GPU support for the counter object used to count occurences of a predefined key-set in a sample.

\subsubsection{Assessment}
To assess whether our CuPy-based GPU accelerated npstructures hash table resulted in speed up compared to the NumPy-based CPU based version, we set up a \textit{k}mer counting benchmark.
A predefined set of 50 million unique \textit{k}mers were to have their frequencies counted from a FASTA file of 20 million reads of length 150.
During benchmarking, the FASTA file was read in chunks of 10 million bytes until completion.
Each chunk would then be 2-bit encoded and hashed using BioNumPy's functionality (on the CPU), before the chunk of hashed \textit{k}mers were copied from the CPU to GPU memory to be counted using the hash table.

We measured the total runtime it took to read the FASTA, 2-bit encode, hash and count the \textit{k}mers for both the original NumPy-based and the new CuPy-based solutions.
For the CuPy solution, this introduced the overhead of copying the hashed \textit{k}mer chunks to the GPU before counting could begin - an inevitable overhead when utilizing the GPU.
For the original NumPy solution running on the CPU, only one thread was used.
The benchmarking was performed on a consumer desktop computer with an Intel Core i5-11400F CPU with 6 cores and a Nvidia GTX 1660 SUPER GPU.

\begin{figure}[H]
%\begin{center}
\hspace*{7em}
\begin{tikzpicture}[font=\small]
  \pgfplotsset{
    compat=newest,
    xlabel near ticks,
    ylabel near ticks
  }
  \pgfplotsset{compat=1.11,
      /pgfplots/ybar legend/.style={
      /pgfplots/legend image code/.code={%
         \draw[##1,/tikz/.cd,yshift=-0.25em]
          (0cm,0cm) rectangle (3pt,0.8em);},
     },
  }
  \node at(3.37,7.5)(){\textbf{Effect of GPU Accelerating npstructures'}};
  \node at(3,7)(){\textbf{NumPy-based hash table using CuPy}};
 
\begin{axis} [
  ylabel={runtime (seconds)},
  xlabel={backend},
  ybar,
  bar width=20pt,
  ymin=0,
  xtick=data,
  axis x line=bottom,
  axis y line=left,
  enlarge x limits=1.25,
  symbolic x coords={NumPy, CuPy},
  xticklabel style={anchor=base, yshift=-\baselineskip},
  /pgf/number format/.cd,fixed,precision=3,
  nodes near coords={\small\pgfmathprintnumber{\pgfplotspointmeta}},
  legend style={anchor=west},
]

\addplot[fill=blue] coordinates {
    (NumPy, 1)
    (CuPy, 1)
};
\end{axis}
\end{tikzpicture}
%\end{center}
\caption{
  Elapsed time counting the occurences of 50 million unique \textit{k}mers in a set of 20 million reads, each of 150 bases.
  Both methods use BioNumPy to read, 2-bit encode and hash chunks of \textit{k}mers, reading 10 million byte chunks from the FASTA at a time.
  Both methods also use npstructures' NumPy-based counter object to count the \textit{k}mer frequencies, although the CuPy-based version replaces NumPy with CuPy to achieve GPU acceleration.
}
\label{methods:initial_testing:figures:benchmark}
\end{figure}

As can be seen in figure \ref{methods:initial_testing:figures:benchmark}, only GPU accelerating the counter object used for \textit{k}mer counting yielded significant improvement in runtime.

...

%\definecolor{cpucolor}{RGB}{255,255,225}
%\definecolor{gpucolor}{RGB}{225,255,255}

%\begin{figure}[H]
%\begin{center}
%\begin{tikzpicture}
%  \node at(6.75,3.75)[]{\smaller{CPU}};
%  \node at(6.75,2.75)[]{\smaller{GPU}};
%  \node at(5.8,3.75)[draw,minimum width=.75cm,minimum height=.75cm,fill=cpucolor]{};
%  \node at(5.8,2.75)[draw,minimum width=.75cm,minimum height=.75cm,fill=gpucolor]{};
%  % -- CPU --
%  \node at(.85,1.5)[]{\smaller{\textbf{Original NumPy pipeline}}};
%  % read fasta
%  \node at(0,0)[draw,minimum width=2cm,minimum height=1.2cm,fill=cpucolor](start){};
%  \node at(0,.25)[]{\smaller{read}};
%  \node at(0,-.25)[]{\smaller{FASTA}};
%  \draw [thick,-stealth](1.25,0) -- (2,0);
%  % 2-bit encoding
%  \node at(3.3,0)[draw,minimum width=2cm,minimum height=1.2cm,fill=cpucolor]{};
%  \node at(3.3,.25)[]{\smaller{2-bit}};
%  \node at(3.3,-.25)[]{\smaller{encoding}};
%  \draw [thick,-stealth](4.65,0) -- (5.4,0);
%  % kmer hashing
%  \node at(6.65,0)[draw,minimum width=2cm,minimum height=1.2cm,fill=cpucolor]{};
%  \node at(6.65,.25)[]{\smaller{\textit{k}mer}};
%  \node at(6.65,-.25)[]{\smaller{hashing}};
%  \draw [thick,-stealth](8,0) -- (8.75,0);
%  % kmer counting 
%  \node at(10.05,0)[draw,minimum width=2cm,minimum height=1.2cm,fill=cpucolor]{};
%  \node at(10.05,.25)[]{\smaller{\textit{k}mer}};
%  \node at(10.05,-.25)[]{\smaller{counting}};
%  % arrows
%  \draw [thick](10.05,-.95) -- (10.05,-1.75);
%  \draw [thick](10.05,-1.75) -- (0,-1.75);
%  \draw [thick,-stealth](0,-1.75) -- (0,-.95);
%  \node at(4.975,-2.25)[]{\smaller{repeat}};
%  % -- GPU --
%  \node at(.8,-3.5)[]{\smaller{\textbf{GPU hash table pipeline}}};
%  % read fasta
%  \node at(0,-5)[draw,minimum width=2cm,minimum height=1.2cm,fill=cpucolor](start){};
%  \node at(0,-4.75)[]{\smaller{read}};
%  \node at(0,-5.25)[]{\smaller{FASTA}};
%  \draw [thick,-stealth](1.25,-5) -- (2,-5);
%  % 2-bit encoding
%  \node at(3.3,-5)[draw,minimum width=2cm,minimum height=1.2cm,fill=cpucolor]{};
%  \node at(3.3,-4.75)[]{\smaller{2-bit}};
%  \node at(3.3,-5.25)[]{\smaller{encoding}};
%  \draw [thick,-stealth](4.65,-5) -- (5.4,-5);
%  % kmer hashing
%  \node at(6.65,-5)[draw,minimum width=2cm,minimum height=1.2cm,fill=cpucolor]{};
%  \node at(6.65,-4.75)[]{\smaller{\textit{k}mer}};
%  \node at(6.65,-5.25)[]{\smaller{hashing}};
%  \draw [thick,-stealth](8,-5) -- (8.75,-5);
%  % cpu2gpu copy
%  \node at(10.05,-5)[draw,minimum width=2cm,minimum height=1.2cm,fill=cpucolor]{};
%  \node at(10.05,-4.75)[]{\smaller{cpu2gpu}};
%  \node at(10.05,-5.25)[]{\smaller{copy}};
%  \draw [thick,-stealth](11.4,-5) -- (12.05,-5);
%  % kmer counting 
%  \node at(13.35,-5)[draw,minimum width=2cm,minimum height=1.2cm,fill=gpucolor]{};
%  \node at(13.35,-4.75)[]{\smaller{\textit{k}mer}};
%  \node at(13.35,-5.25)[]{\smaller{counting}};
%  % arrows
%  \draw [thick](13.35,-5.95) -- (13.35,-6.75);
%  \draw [thick](13.35,-6.75) -- (0,-6.75);
%  \draw [thick,-stealth](0,-6.75) -- (0,-5.95);
%  \node at(6.65,-7.25)[]{\smaller{repeat}};
%\end{tikzpicture}
%\end{center}
%\end{figure}
